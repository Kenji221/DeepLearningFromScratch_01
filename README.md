# About Perceptron
## What are perceptrons?
Input a certain amount of number and get an output depending on the theta of the value 

## AND / OR / NOTOR
### AND GATE
x | y | OUTPUT
0 | 0 | 0
1 | 0 | 0
0 | 1 | 0
1 | 1 | 1

### NOTAND GATEでコンピュータが作れる



# Nerual Network
## ニューラルネット / ニューロン / 活性化関数について
- ニューラルネット：人間の脳の神経回路を模したモデルです。
入力層、隠れ層、出力層といった階層で構成され、情報を段階的に処理します。
各層は複数のニューロン（ノード）で構成されており、これらのニューロンが相互に情報をやり取りします。﻿

- ニューロン：入力に重みを乗算し、バイアスを加えるという計算を行い、その結果を活性化関数に渡します。活性化関数は、このニューロンの最終的な出力を決定する役割を担います。

- 活性化関数：ニューロンへのすべての入力を、次のニューロンへの出力に変換する関数

- ニューロンとパーセプトロンの違い：ニューロンは実際の生物学的に電気信号を受け取って0~1の値を出力することを指す
一方でパーセプトロンは重みや数字を入力して出力を決めたものを指している

## 関数について
### 線形と非線形関数について
ニューラルネットにおいて、線形関数は隠れ層に代入しても結局線形の形で表すことができるため、不要となる。  
### 様々な関数
- STEP関数：0以前の数字については０を出力して、０より大きい数は１を出力する（ステップ関数とも呼ぶ）
- Sigmoid関数: 1/(1+e)
- Relu関数：0までは０を出力し、そのあとはy=Xを出力する

## 行列の積について
しっかり配列の次元数があっていないと積が求められないことに注意すること

### ニューラルネットの実装
★TODO: コーディングしてみよう

## 恒等関数とソフトマックス関数
- 恒等関数：入力層をそのままの形で出力する。回帰モデルに便利
- ソフトマックス関数：exp(ak)/sum(exp(ai)): 総和が１になり、分類モデルに活用しやすくなる

## MNISTを使った予測
### 画像の読み込み
実際にMNISTの画像を読み込んで、一次元の圧縮や

### ニューラルネットの実装
- 入力層：画像を一次元に圧縮した時の値
- 中間層：実際の配列の数が合うように調整した中間層の数の値
- 出力層：予測したい数（1/10）の数字のため１０個の出力層  
実際の入力については0~1の数字で収める必要があり、そのNormalizeをする必要がある。


