# TODO
- e（自然対数）についてより深い知識を獲得せよ
- BatchNormの実装をせよ
- DropOutの実装をせよ
- weight Decayの実装をせよ
- Convolution / Poolingの逆伝搬の計算


# About Perceptron
## What are perceptrons?
Input a certain amount of number and get an output depending on the theta of the value 

## AND / OR / NOTOR
### AND GATE
x | y | OUTPUT
0 | 0 | 0
1 | 0 | 0
0 | 1 | 0
1 | 1 | 1

### NOTAND GATEでコンピュータが作れる

## 実装
- GATES.py


# Nerual Network
## ニューラルネット / ニューロン / 活性化関数について
- ニューラルネット：人間の脳の神経回路を模したモデルです。
入力層、隠れ層、出力層といった階層で構成され、情報を段階的に処理します。
各層は複数のニューロン（ノード）で構成されており、これらのニューロンが相互に情報をやり取りします。﻿

- ニューロン：入力に重みを乗算し、バイアスを加えるという計算を行い、その結果を活性化関数に渡します。活性化関数は、このニューロンの最終的な出力を決定する役割を担います。

- 活性化関数：ニューロンへのすべての入力を、次のニューロンへの出力に変換する関数

- ニューロンとパーセプトロンの違い：ニューロンは実際の生物学的に電気信号を受け取って0~1の値を出力することを指す
一方でパーセプトロンは重みや数字を入力して出力を決めたものを指している

- 第一章での式の表し方について   
beta + x1w1 + x2w2 < 0   
beta + x1w1 + x2w2 >= 0    
などで表す事ができる

この場合beta をbias w1,w2を重みと呼ぶ.   


## 関数について
### 線形と非線形関数について
ニューラルネットにおいて、線形関数は隠れ層に代入しても結局線形の形で表すことができるため、不要となる。  
### 様々な関数
- STEP関数：0以前の数字については０を出力して、０より大きい数は１を出力する（ステップ関数とも呼ぶ）
- Sigmoid関数: 1/(1+e)
- Relu関数：0までは０を出力し、そのあとはy=Xを出力する

#### 関数の実装
★ ch3 Activation_Function.py

## 行列の積について
しっかり配列の次元数があっていないと積が求められないことに注意すること.   
[[1,2],  
 [3,4]].     
[[5,6],       
 [7,8]]. 

[2,2] x [2,2] =[2,2]

[1*5+2*7,1*6+2*8],
[3*5+4*7,3*6+4*8]
[19,22],
[43,50]

→左側の横の列すうと右の行の数が一致しないといけない
(例)[3,2] * [2,3]の行列
→[3,3]のアウトプットとなる
[a,b], 
[c,d],
[e,f] 
[g,h,i],
[j,k,l]
[2,3] * [3,2]
[a*g+b*j,a*h+b*k,a*i+b*l],
[c+b*j,a*h+b*k,a*i+b*l]

## 恒等関数とソフトマックス関数
- 恒等関数：入力層をそのままの形で出力する。回帰モデルに便利
- ソフトマックス関数：exp(ak)/sum(exp(ai)): 総和が１になり、分類モデルに活用しやすくなる
TODO: ソフトマック関数の数学的な意味をきちんと理解する / なぜ自然対数を使っているのか
### 行列の積とニューラルネットと活性化関数 put together

## MNISTを使った予測
### 画像の読み込み
MNIST画像：機械学習によく用いられている白黒の数字が書かれているデータを表している
- 読み込み手順について
    - Mnistのデータ自体がバイナリデータとなっている
    - numpy.arrayにバイナリデータを読み込む
        - 画像として読み込みたい場合はpython のIMGを使う (mnist_show.py) 
        - それぞれ機械学習で扱いやすいようにNormalizeやFlattenを行うと良い 


#### np.frombufferについて
Pythonが得意とする科学技術計算のアルゴリズムはNumPyもそうですが、高速化のためにCやFortranなどのコンパイルされた低レベルなコードが動いていることが多いです。このようなネイティブコードとPythonコードとをやりとりするために、Buffer Protocolという機構がPythonには備わっています。

### ニューラルネットの実装
- 入力層：画像を一次元に圧縮した時の値
- 中間層：実際の配列の数が合うように調整した中間層の数の値
- 出力層：予測したい数（1/10）の数字のため１０個の出力層  
実際の入力については0~1の数字で収める必要があり、そのNormalizeをする必要がある。


# ニューラルネットの学習について
## DeepLearningと機械学習の違いについて
- 機械学習：人が目的に応じた特徴量を作成して機械が答えを出すような内容
- DeepLearning：機会が入力データから規則を出して答えを出すようなもの


## 損失関数とは
直感的：ある指標において、その指標に対してどれくらいの状態かで表すこと
損失関数は予測と実際の値のズレの大きさを表す関数で、損失関数の値が小さければより正確なモデルと言えます。

### 損失関数の具体例
- 交差エントロピー
- Mean Squared Error (MSE)
### なぜ損失関数を使うのか？
元の評価指標(数字の画像の判定であれば、その判定指標)を損失関数として使わない理由としては変化量（微分係数）が横ばいになってしまう傾向にあるから

### 微分と偏微分について
微分とは　lim h→0 f(x+h)-f(x) / h
偏微分について、多変数関数において、注目している変数以外のすべての変数を定数とみなして、その1つの変数だけについて微分すること

### 　勾配法について

- 勾配とは：ある関数における全ての偏微分をベクトルとしてまとめたものです。 
- 学習率：勾配法において、更新するステップ数の大きさを表している
- 勾配効果法: 多変量関数において、勾配から全て傾きが最小の方向に向かうアルゴリズムを指している

https://avilen.co.jp/personal/knowledge-article/optimizer/

## 勾配法とニューラルネット
重みとバイアスを定めるために勾配法を用いて、それぞれの変数が損失関数に対して最小の値を取るようにを学習率を調整して最小の値となるように調整していくことを指していく

TODO: 勾配法の実装 / 勾配法のミニバッチ、確率的勾配など　それぞれの最適化を行なっていく　それぞれMnistを使って実装していこう
https://www.ibm.com/jp-ja/think/topics/gradient-descent


## コラム
- エポック数：１エポックとはデータに対して全てのデータを使い切った時の回数を指している


